{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import commonly used libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number words female</th>\n",
       "      <th>Total words</th>\n",
       "      <th>Number of words lead</th>\n",
       "      <th>Difference in words lead and co-lead</th>\n",
       "      <th>Number of male actors</th>\n",
       "      <th>Year</th>\n",
       "      <th>Number of female actors</th>\n",
       "      <th>Number words male</th>\n",
       "      <th>Gross</th>\n",
       "      <th>Mean Age Male</th>\n",
       "      <th>Mean Age Female</th>\n",
       "      <th>Age Lead</th>\n",
       "      <th>Age Co-Lead</th>\n",
       "      <th>Lead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1512</td>\n",
       "      <td>6394</td>\n",
       "      <td>2251.0</td>\n",
       "      <td>343</td>\n",
       "      <td>2</td>\n",
       "      <td>1995</td>\n",
       "      <td>5</td>\n",
       "      <td>2631</td>\n",
       "      <td>142.0</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>42.333333</td>\n",
       "      <td>46.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1524</td>\n",
       "      <td>8780</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>1219</td>\n",
       "      <td>9</td>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>5236</td>\n",
       "      <td>37.0</td>\n",
       "      <td>39.125000</td>\n",
       "      <td>29.333333</td>\n",
       "      <td>58.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>155</td>\n",
       "      <td>4176</td>\n",
       "      <td>942.0</td>\n",
       "      <td>787</td>\n",
       "      <td>7</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>3079</td>\n",
       "      <td>376.0</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>46.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1073</td>\n",
       "      <td>9855</td>\n",
       "      <td>3440.0</td>\n",
       "      <td>2623</td>\n",
       "      <td>12</td>\n",
       "      <td>2002</td>\n",
       "      <td>2</td>\n",
       "      <td>5342</td>\n",
       "      <td>19.0</td>\n",
       "      <td>35.222222</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>33.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1317</td>\n",
       "      <td>7688</td>\n",
       "      <td>3835.0</td>\n",
       "      <td>3149</td>\n",
       "      <td>8</td>\n",
       "      <td>1988</td>\n",
       "      <td>4</td>\n",
       "      <td>2536</td>\n",
       "      <td>40.0</td>\n",
       "      <td>45.250000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number words female  Total words  Number of words lead  \\\n",
       "0                 1512         6394                2251.0   \n",
       "1                 1524         8780                2020.0   \n",
       "2                  155         4176                 942.0   \n",
       "3                 1073         9855                3440.0   \n",
       "4                 1317         7688                3835.0   \n",
       "\n",
       "   Difference in words lead and co-lead  Number of male actors  Year  \\\n",
       "0                                   343                      2  1995   \n",
       "1                                  1219                      9  2001   \n",
       "2                                   787                      7  1968   \n",
       "3                                  2623                     12  2002   \n",
       "4                                  3149                      8  1988   \n",
       "\n",
       "   Number of female actors  Number words male  Gross  Mean Age Male  \\\n",
       "0                        5               2631  142.0      51.500000   \n",
       "1                        4               5236   37.0      39.125000   \n",
       "2                        1               3079  376.0      42.500000   \n",
       "3                        2               5342   19.0      35.222222   \n",
       "4                        4               2536   40.0      45.250000   \n",
       "\n",
       "   Mean Age Female  Age Lead  Age Co-Lead    Lead  \n",
       "0        42.333333      46.0         65.0  Female  \n",
       "1        29.333333      58.0         34.0    Male  \n",
       "2        37.000000      46.0         37.0    Male  \n",
       "3        21.500000      33.0         23.0    Male  \n",
       "4        45.000000      36.0         39.0    Male  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and test set \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = df.drop(['Lead'], axis=1)\n",
    "labels = df['Lead']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     features, labels, train_size=0.7\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminant Analysis: \n",
    "- ### LDA & QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score LDA: 0.8525641025641025\n",
      "Score QDA: 0.8910256410256411\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    " \n",
    "    LDA_clf = LinearDiscriminantAnalysis()\n",
    "    QDA_clf = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "    LDA_clf.fit(X_train, y_train)\n",
    "    QDA_clf.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "score_LDA = LDA_clf.score(X_test, y_test)\n",
    "score_QDA = QDA_clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Score LDA: \" + str(score_LDA))\n",
    "print(\"Score QDA: \" + str(score_QDA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper tuning and Tenfold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m LINEAR DISCRIMINANT ANALYSIS \u001b[0m\n",
      "\n",
      "Mean Accuracy: 0.860\n",
      "Config: {'solver': 'svd'}\n",
      "Score: 0.8621794871794872\n",
      "\n",
      "\u001b[1m QUADRATIC DISCRIMINANT ANALYSIS \u001b[0m\n",
      "\n",
      "Mean Accuracy: 0.898\n",
      "Config: {'reg_param': 0.2}\n",
      "Score: 0.8910256410256411\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold, KFold, GridSearchCV\n",
    "\n",
    "\n",
    "rs_kf = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "# param_grids to search\n",
    "params_LDA = [{'solver': ['svd', 'lsqr', 'eigen']}]\n",
    "params_QDA = [{'reg_param': [0.1, 0.2, 0.3, 0.31, 0.4, 0.5]}]\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # define search\n",
    "    gs_LDA = GridSearchCV(LDA_clf, params_LDA, scoring='accuracy', cv=10)\n",
    "    gs_QDA = GridSearchCV(QDA_clf, params_QDA, scoring='accuracy', cv=10) \n",
    "\n",
    "# perform search\n",
    "    LDA_results = gs_LDA.fit(X_train, y_train)\n",
    "    QDA_results = gs_QDA.fit(X_train, y_train)\n",
    "\n",
    "# calculate scores \n",
    "score_LDA = LDA_results.score(X_test, y_test)\n",
    "score_QDA = QDA_results.score(X_test, y_test)\n",
    "\n",
    "# print findings\n",
    "print(\"\\033[1m LINEAR DISCRIMINANT ANALYSIS \\033[0m\\n\")\n",
    "print('Mean Accuracy: %.3f' % LDA_results.best_score_)\n",
    "print('Config: %s' % LDA_results.best_params_)\n",
    "print(\"Score: \" + str(score_LDA) + \"\\n\")\n",
    "\n",
    "print(\"\\033[1m QUADRATIC DISCRIMINANT ANALYSIS \\033[0m\\n\")\n",
    "print('Mean Accuracy: %.3f' % QDA_results.best_score_)\n",
    "print('Config: %s' % QDA_results.best_params_)\n",
    "print(\"Score: \" + str(score_QDA) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "classifiers = {\n",
    "    'Linear Discrminant Analysis': gs_LDA,\n",
    "    'Quadratic Discrminant Analysis': gs_QDA\n",
    "}\n",
    "\n",
    "cf_dataframe = dict.fromkeys(classifiers.keys())\n",
    "for key, classifier in classifiers.items(): \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        y_pred = classifier.fit(X_train, y_train.values.ravel()).predict(X_test)\n",
    "    \n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    cf_dataframe[key]= pd.DataFrame(\n",
    "        cf_matrix, index = ['Female', 'Male'], columns = ['Female', 'Male']\n",
    "    )\n",
    "    \n",
    "\n",
    "####### PLOT ########\n",
    "\n",
    "fig, axn = plt.subplots(1, len(classifiers), sharex=True, sharey=True, figsize=(15,5))\n",
    "\n",
    "for i, ax in enumerate(axn.flat):\n",
    "    k = list(cf_dataframe)[i]\n",
    "    sns.heatmap(cf_dataframe[k], ax=ax, annot=True, fmt=\"d\", annot_kws={\"size\": 14}, cmap='Blues')\n",
    "    ax.set_title(k, fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number words female</th>\n",
       "      <th>Total words</th>\n",
       "      <th>Number of words lead</th>\n",
       "      <th>Difference in words lead and co-lead</th>\n",
       "      <th>Number of male actors</th>\n",
       "      <th>Year</th>\n",
       "      <th>Number of female actors</th>\n",
       "      <th>Number words male</th>\n",
       "      <th>Gross</th>\n",
       "      <th>Mean Age Male</th>\n",
       "      <th>Mean Age Female</th>\n",
       "      <th>Age Lead</th>\n",
       "      <th>Age Co-Lead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1512</td>\n",
       "      <td>6394</td>\n",
       "      <td>2251.0</td>\n",
       "      <td>343</td>\n",
       "      <td>2</td>\n",
       "      <td>1995</td>\n",
       "      <td>5</td>\n",
       "      <td>2631</td>\n",
       "      <td>142.0</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>42.333333</td>\n",
       "      <td>46.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1524</td>\n",
       "      <td>8780</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>1219</td>\n",
       "      <td>9</td>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>5236</td>\n",
       "      <td>37.0</td>\n",
       "      <td>39.125000</td>\n",
       "      <td>29.333333</td>\n",
       "      <td>58.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>155</td>\n",
       "      <td>4176</td>\n",
       "      <td>942.0</td>\n",
       "      <td>787</td>\n",
       "      <td>7</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>3079</td>\n",
       "      <td>376.0</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>46.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1073</td>\n",
       "      <td>9855</td>\n",
       "      <td>3440.0</td>\n",
       "      <td>2623</td>\n",
       "      <td>12</td>\n",
       "      <td>2002</td>\n",
       "      <td>2</td>\n",
       "      <td>5342</td>\n",
       "      <td>19.0</td>\n",
       "      <td>35.222222</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>33.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1317</td>\n",
       "      <td>7688</td>\n",
       "      <td>3835.0</td>\n",
       "      <td>3149</td>\n",
       "      <td>8</td>\n",
       "      <td>1988</td>\n",
       "      <td>4</td>\n",
       "      <td>2536</td>\n",
       "      <td>40.0</td>\n",
       "      <td>45.250000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>303</td>\n",
       "      <td>2398</td>\n",
       "      <td>1334.0</td>\n",
       "      <td>1166</td>\n",
       "      <td>5</td>\n",
       "      <td>1973</td>\n",
       "      <td>2</td>\n",
       "      <td>761</td>\n",
       "      <td>174.0</td>\n",
       "      <td>43.200000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>46.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>632</td>\n",
       "      <td>8404</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>187</td>\n",
       "      <td>6</td>\n",
       "      <td>1992</td>\n",
       "      <td>2</td>\n",
       "      <td>5820</td>\n",
       "      <td>172.0</td>\n",
       "      <td>37.166667</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>1326</td>\n",
       "      <td>2750</td>\n",
       "      <td>877.0</td>\n",
       "      <td>356</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>3</td>\n",
       "      <td>547</td>\n",
       "      <td>53.0</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>27.666667</td>\n",
       "      <td>28.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>462</td>\n",
       "      <td>3994</td>\n",
       "      <td>775.0</td>\n",
       "      <td>52</td>\n",
       "      <td>8</td>\n",
       "      <td>1996</td>\n",
       "      <td>3</td>\n",
       "      <td>2757</td>\n",
       "      <td>32.0</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>2735</td>\n",
       "      <td>11946</td>\n",
       "      <td>3410.0</td>\n",
       "      <td>1536</td>\n",
       "      <td>13</td>\n",
       "      <td>2007</td>\n",
       "      <td>4</td>\n",
       "      <td>5801</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.090909</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>38.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1039 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Number words female  Total words  Number of words lead  \\\n",
       "0                    1512         6394                2251.0   \n",
       "1                    1524         8780                2020.0   \n",
       "2                     155         4176                 942.0   \n",
       "3                    1073         9855                3440.0   \n",
       "4                    1317         7688                3835.0   \n",
       "...                   ...          ...                   ...   \n",
       "1034                  303         2398                1334.0   \n",
       "1035                  632         8404                1952.0   \n",
       "1036                 1326         2750                 877.0   \n",
       "1037                  462         3994                 775.0   \n",
       "1038                 2735        11946                3410.0   \n",
       "\n",
       "      Difference in words lead and co-lead  Number of male actors  Year  \\\n",
       "0                                      343                      2  1995   \n",
       "1                                     1219                      9  2001   \n",
       "2                                      787                      7  1968   \n",
       "3                                     2623                     12  2002   \n",
       "4                                     3149                      8  1988   \n",
       "...                                    ...                    ...   ...   \n",
       "1034                                  1166                      5  1973   \n",
       "1035                                   187                      6  1992   \n",
       "1036                                   356                      2  2000   \n",
       "1037                                    52                      8  1996   \n",
       "1038                                  1536                     13  2007   \n",
       "\n",
       "      Number of female actors  Number words male  Gross  Mean Age Male  \\\n",
       "0                           5               2631  142.0      51.500000   \n",
       "1                           4               5236   37.0      39.125000   \n",
       "2                           1               3079  376.0      42.500000   \n",
       "3                           2               5342   19.0      35.222222   \n",
       "4                           4               2536   40.0      45.250000   \n",
       "...                       ...                ...    ...            ...   \n",
       "1034                        2                761  174.0      43.200000   \n",
       "1035                        2               5820  172.0      37.166667   \n",
       "1036                        3                547   53.0      27.500000   \n",
       "1037                        3               2757   32.0      42.857143   \n",
       "1038                        4               5801   32.0      44.090909   \n",
       "\n",
       "      Mean Age Female  Age Lead  Age Co-Lead  \n",
       "0           42.333333      46.0         65.0  \n",
       "1           29.333333      58.0         34.0  \n",
       "2           37.000000      46.0         37.0  \n",
       "3           21.500000      33.0         23.0  \n",
       "4           45.000000      36.0         39.0  \n",
       "...               ...       ...          ...  \n",
       "1034        31.000000      46.0         24.0  \n",
       "1035        24.000000      21.0         34.0  \n",
       "1036        27.666667      28.0         25.0  \n",
       "1037        38.500000      29.0         32.0  \n",
       "1038        50.000000      38.0         48.0  \n",
       "\n",
       "[1039 rows x 13 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(['Lead'], axis=1)\n",
    "labels = df['Lead']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     features, labels, train_size=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, chain\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def find_best_subset(estimator, X, y, max_size=15, cv=10, print_progress=True):\n",
    "    \"\"\"\n",
    "    Calculates the best model of up to max_size features of X.\n",
    "    estimator must have a fit and score functions.\n",
    "    X must be a DataFrame.\n",
    "    Source of function: https://stackoverflow.com/a/50704252/6400551\n",
    "    \"\"\"\n",
    "\n",
    "    n_features = X.shape[1]\n",
    "    subsets = (combinations(range(n_features), k + 1)\n",
    "               for k in range(min(n_features, max_size)))\n",
    "\n",
    "    subsets_2 = (combinations(range(n_features), k + 1)\n",
    "                 for k in range(min(n_features, max_size)))\n",
    "\n",
    "    best_size_subset = []\n",
    "\n",
    "    progress_percentage = 0\n",
    "    progress = 0\n",
    "    # total_combinations = sum(math.comb(n_features, size) for size in range(max_size + 1))\n",
    "    total_combinations = 0\n",
    "\n",
    "    for subsets_k in subsets_2:\n",
    "        for subset in subsets_k:\n",
    "            total_combinations += 1\n",
    "\n",
    "    if print_progress:\n",
    "        print(f\"Looking through {total_combinations} combinations...\")\n",
    "\n",
    "    for subsets_k in subsets:  # for each list of subsets of the same size\n",
    "        best_score = -np.inf\n",
    "        best_subset = None\n",
    "\n",
    "        for subset in subsets_k:\n",
    "            if print_progress:\n",
    "                progress += 1\n",
    "\n",
    "                percentage = 100.0 * (progress / float(total_combinations))\n",
    "                percentage_int = int(percentage)\n",
    "\n",
    "                #if percentage_int > progress_percentage:\n",
    "                #    progress_percentage = percentage_int\n",
    "                #    print(f\"Progress: {progress_percentage}%\")\n",
    "\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                estimator.fit(X.iloc[:, list(subset)], y)\n",
    "            # get the subset with the best score among subsets of the same size\n",
    "                score = estimator.score(X.iloc[:, list(subset)], y)\n",
    "            if score > best_score:\n",
    "                best_score, best_subset = score, subset\n",
    "\n",
    "        # to compare subsets of different sizes we must use CV\n",
    "        # first store the best subset of each size\n",
    "        best_size_subset.append(best_subset)\n",
    "\n",
    "    # compare best subsets of each size\n",
    "    best_score = -np.inf\n",
    "    best_subset = None\n",
    "    list_scores = []\n",
    "    for subset in best_size_subset:\n",
    "        score = cross_val_score(estimator, X.iloc[:, list(subset)], y, cv=cv).mean()\n",
    "        list_scores.append(score)\n",
    "        if score > best_score:\n",
    "            best_score, best_subset = score, subset\n",
    "\n",
    "    return best_subset, best_score, best_size_subset, list_scores\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking through 8191 combinations...\n",
      "Best subset: (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12)\n",
      "Best score: 0.9047143390589995\n",
      "Best size subset: [(6,), (4, 6), (0, 2, 3), (0, 2, 3, 6), (0, 2, 3, 4, 6), (0, 1, 2, 3, 4, 6), (0, 1, 2, 3, 4, 6, 11), (0, 1, 2, 3, 4, 6, 10, 11), (0, 1, 2, 3, 4, 5, 6, 10, 11), (0, 1, 2, 3, 4, 6, 8, 9, 10, 11), (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12), (0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12), (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)]\n",
      "List of scores: [0.7632468259895444, 0.7738610903659447, 0.8132561613144137, 0.853696788648245, 0.8700802837938759, 0.8835324869305452, 0.8960324869305453, 0.8979742345033607, 0.8921956684092607, 0.8979835698282301, 0.9047143390589995, 0.900858849887976, 0.8604088872292757]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jonathan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\users\\jonathan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\users\\jonathan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\users\\jonathan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\users\\jonathan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\users\\jonathan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\users\\jonathan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\users\\jonathan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\users\\jonathan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\users\\jonathan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "best_set, best_score, best_size_subset, list_scores = find_best_subset(QuadraticDiscriminantAnalysis(), features, labels)\n",
    "\n",
    "print(f\"Best subset: {best_set}\")\n",
    "print(f\"Best score: {best_score}\")\n",
    "print(f\"Best size subset: {best_size_subset}\")\n",
    "print(f\"List of scores: {list_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-47-6d4a5074b1c7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-47-6d4a5074b1c7>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Jonathan LDA: 0, 1, 2, 3, 4, 6, 9, 11, 12\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Jonathan LDA: 0, 1, 2, 3, 4, 6, 9, 11, 12\n",
    "Jonathan QDA: 0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12\n",
    "Simon: 0, 2, 3, 8\n",
    "Benny: 0, 1, 2, 3, 4, 6, 11, 12\n",
    "Gholam: 0, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Number words female', 'Number of words lead', 'Difference in words lead and co-lead', 'Number of male actors', 'Number of female actors', 'Number words male', 'Gross', 'Mean Age Male', 'Mean Age Female', 'Age Lead', 'Age Co-Lead']\n"
     ]
    }
   ],
   "source": [
    "best_subset = get_best_subset([0, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12])\n",
    "print(best_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number words female</th>\n",
       "      <th>Total words</th>\n",
       "      <th>Number of words lead</th>\n",
       "      <th>Difference in words lead and co-lead</th>\n",
       "      <th>Number of male actors</th>\n",
       "      <th>Year</th>\n",
       "      <th>Number of female actors</th>\n",
       "      <th>Number words male</th>\n",
       "      <th>Gross</th>\n",
       "      <th>Mean Age Male</th>\n",
       "      <th>Mean Age Female</th>\n",
       "      <th>Age Lead</th>\n",
       "      <th>Age Co-Lead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1512</td>\n",
       "      <td>6394</td>\n",
       "      <td>2251.0</td>\n",
       "      <td>343</td>\n",
       "      <td>2</td>\n",
       "      <td>1995</td>\n",
       "      <td>5</td>\n",
       "      <td>2631</td>\n",
       "      <td>142.0</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>42.333333</td>\n",
       "      <td>46.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1524</td>\n",
       "      <td>8780</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>1219</td>\n",
       "      <td>9</td>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>5236</td>\n",
       "      <td>37.0</td>\n",
       "      <td>39.125000</td>\n",
       "      <td>29.333333</td>\n",
       "      <td>58.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>155</td>\n",
       "      <td>4176</td>\n",
       "      <td>942.0</td>\n",
       "      <td>787</td>\n",
       "      <td>7</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>3079</td>\n",
       "      <td>376.0</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>46.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1073</td>\n",
       "      <td>9855</td>\n",
       "      <td>3440.0</td>\n",
       "      <td>2623</td>\n",
       "      <td>12</td>\n",
       "      <td>2002</td>\n",
       "      <td>2</td>\n",
       "      <td>5342</td>\n",
       "      <td>19.0</td>\n",
       "      <td>35.222222</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>33.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1317</td>\n",
       "      <td>7688</td>\n",
       "      <td>3835.0</td>\n",
       "      <td>3149</td>\n",
       "      <td>8</td>\n",
       "      <td>1988</td>\n",
       "      <td>4</td>\n",
       "      <td>2536</td>\n",
       "      <td>40.0</td>\n",
       "      <td>45.250000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>303</td>\n",
       "      <td>2398</td>\n",
       "      <td>1334.0</td>\n",
       "      <td>1166</td>\n",
       "      <td>5</td>\n",
       "      <td>1973</td>\n",
       "      <td>2</td>\n",
       "      <td>761</td>\n",
       "      <td>174.0</td>\n",
       "      <td>43.200000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>46.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>632</td>\n",
       "      <td>8404</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>187</td>\n",
       "      <td>6</td>\n",
       "      <td>1992</td>\n",
       "      <td>2</td>\n",
       "      <td>5820</td>\n",
       "      <td>172.0</td>\n",
       "      <td>37.166667</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>1326</td>\n",
       "      <td>2750</td>\n",
       "      <td>877.0</td>\n",
       "      <td>356</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>3</td>\n",
       "      <td>547</td>\n",
       "      <td>53.0</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>27.666667</td>\n",
       "      <td>28.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>462</td>\n",
       "      <td>3994</td>\n",
       "      <td>775.0</td>\n",
       "      <td>52</td>\n",
       "      <td>8</td>\n",
       "      <td>1996</td>\n",
       "      <td>3</td>\n",
       "      <td>2757</td>\n",
       "      <td>32.0</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>2735</td>\n",
       "      <td>11946</td>\n",
       "      <td>3410.0</td>\n",
       "      <td>1536</td>\n",
       "      <td>13</td>\n",
       "      <td>2007</td>\n",
       "      <td>4</td>\n",
       "      <td>5801</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.090909</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>38.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1039 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Number words female  Total words  Number of words lead  \\\n",
       "0                    1512         6394                2251.0   \n",
       "1                    1524         8780                2020.0   \n",
       "2                     155         4176                 942.0   \n",
       "3                    1073         9855                3440.0   \n",
       "4                    1317         7688                3835.0   \n",
       "...                   ...          ...                   ...   \n",
       "1034                  303         2398                1334.0   \n",
       "1035                  632         8404                1952.0   \n",
       "1036                 1326         2750                 877.0   \n",
       "1037                  462         3994                 775.0   \n",
       "1038                 2735        11946                3410.0   \n",
       "\n",
       "      Difference in words lead and co-lead  Number of male actors  Year  \\\n",
       "0                                      343                      2  1995   \n",
       "1                                     1219                      9  2001   \n",
       "2                                      787                      7  1968   \n",
       "3                                     2623                     12  2002   \n",
       "4                                     3149                      8  1988   \n",
       "...                                    ...                    ...   ...   \n",
       "1034                                  1166                      5  1973   \n",
       "1035                                   187                      6  1992   \n",
       "1036                                   356                      2  2000   \n",
       "1037                                    52                      8  1996   \n",
       "1038                                  1536                     13  2007   \n",
       "\n",
       "      Number of female actors  Number words male  Gross  Mean Age Male  \\\n",
       "0                           5               2631  142.0      51.500000   \n",
       "1                           4               5236   37.0      39.125000   \n",
       "2                           1               3079  376.0      42.500000   \n",
       "3                           2               5342   19.0      35.222222   \n",
       "4                           4               2536   40.0      45.250000   \n",
       "...                       ...                ...    ...            ...   \n",
       "1034                        2                761  174.0      43.200000   \n",
       "1035                        2               5820  172.0      37.166667   \n",
       "1036                        3                547   53.0      27.500000   \n",
       "1037                        3               2757   32.0      42.857143   \n",
       "1038                        4               5801   32.0      44.090909   \n",
       "\n",
       "      Mean Age Female  Age Lead  Age Co-Lead  \n",
       "0           42.333333      46.0         65.0  \n",
       "1           29.333333      58.0         34.0  \n",
       "2           37.000000      46.0         37.0  \n",
       "3           21.500000      33.0         23.0  \n",
       "4           45.000000      36.0         39.0  \n",
       "...               ...       ...          ...  \n",
       "1034        31.000000      46.0         24.0  \n",
       "1035        24.000000      21.0         34.0  \n",
       "1036        27.666667      28.0         25.0  \n",
       "1037        38.500000      29.0         32.0  \n",
       "1038        50.000000      38.0         48.0  \n",
       "\n",
       "[1039 rows x 13 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jonathan LDA: 0, 1, 2, 3, 4, 6, 9, 11, 12\n",
    "    \n",
    "def get_best_subset(arr_of_features):\n",
    "    subset = df.columns.values.tolist()\n",
    "    subset_to_remove = df.columns.values.tolist()\n",
    "    \n",
    "    for index in sorted(arr_of_features, reverse=True):\n",
    "        del subset_to_remove[index]\n",
    "    \n",
    "    for index in sorted(subset_to_remove, reverse=True):\n",
    "        subset.remove(index)        \n",
    "        \n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_subset = get_best_subset([0, 1, 2, 3, 4, 5, 6, 9, 11, 12])\n",
    "\n",
    "features = df[best_subset]\n",
    "labels = df['Lead']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     features, labels, train_size=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score LDA: 0.8653846153846154\n",
      "Score QDA: 0.9006410256410257\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    " \n",
    "    LDA_clf = LinearDiscriminantAnalysis()\n",
    "    QDA_clf = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "    LDA_clf.fit(X_train, y_train)\n",
    "    QDA_clf.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "score_LDA = LDA_clf.score(X_test, y_test)\n",
    "score_QDA = QDA_clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Score LDA: \" + str(score_LDA))\n",
    "print(\"Score QDA: \" + str(score_QDA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold, KFold, GridSearchCV\n",
    "\n",
    "\n",
    "rs_kf = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "# param_grids to search\n",
    "params_LDA = [{'solver': ['svd', 'lsqr', 'eigen']}]\n",
    "params_QDA = [{'reg_param': [0.1, 0.2, 0.3, 0.31, 0.4, 0.5]}]\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # define search\n",
    "    gs_LDA = GridSearchCV(LDA_clf, params_LDA, scoring='accuracy', cv=10)\n",
    "    gs_QDA = GridSearchCV(QDA_clf, params_QDA, scoring='accuracy', cv=10) \n",
    "\n",
    "# perform search\n",
    "    LDA_results = gs_LDA.fit(X_train, y_train)\n",
    "    QDA_results = gs_QDA.fit(X_train, y_train)\n",
    "\n",
    "# calculate scores \n",
    "score_LDA = LDA_results.score(X_test, y_test)\n",
    "score_QDA = QDA_results.score(X_test, y_test)\n",
    "\n",
    "# print findings\n",
    "print(\"\\033[1m LINEAR DISCRIMINANT ANALYSIS \\033[0m\\n\")\n",
    "print('Mean Accuracy: %.3f' % LDA_results.best_score_)\n",
    "print('Config: %s' % LDA_results.best_params_)\n",
    "print(\"Score: \" + str(score_LDA) + \"\\n\")\n",
    "\n",
    "print(\"\\033[1m QUADRATIC DISCRIMINANT ANALYSIS \\033[0m\\n\")\n",
    "print('Mean Accuracy: %.3f' % QDA_results.best_score_)\n",
    "print('Config: %s' % QDA_results.best_params_)\n",
    "print(\"Score: \" + str(score_QDA) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for Random Forest: 0.146\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "features = df.drop(['Lead'], axis=1)\n",
    "labels = df['Lead']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     features, labels, train_size=0.7\n",
    ")\n",
    "\n",
    "err = 0\n",
    "for i in range(100):\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    err = err + np.mean(pred != y_test)\n",
    "err = err/100\n",
    "print(f'Error rate for Random Forest: {err:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "features = df.drop(['Lead'], axis=1)\n",
    "labels = df['Lead']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     features, labels, train_size=0.7\n",
    ")\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "err = np.mean(pred != y_test)\n",
    "print(f'Error rate for Random Forest: {err:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
